{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Importy, wizualizacja\n",
    "(Należy odpalić i schować)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Subset\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.transforms import Compose, Lambda, ToTensor\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def plot_dataset(train_data, generator, is_cond=False, n_samples = 5, show_train=True):\n",
    "    view_data = train_data.data[:n_samples].view(-1, 28 * 28) / 255.0\n",
    "\n",
    "    if show_train:\n",
    "        labels = train_data.targets[:n_samples]\n",
    "    else:\n",
    "        labels = torch.arange(n_samples) % 10\n",
    "\n",
    "    noise = torch.randn((n_samples, generator.latent_dim), device=device)\n",
    "    with torch.no_grad():\n",
    "        if is_cond:\n",
    "            labels_one_hot = torch.nn.functional.one_hot(labels, 10).to(torch.float32).to(device)\n",
    "            gen_data = generator(noise, labels_one_hot).cpu().detach().numpy()\n",
    "        else:\n",
    "            gen_data = generator(noise).cpu().detach().numpy()\n",
    "\n",
    "    n_rows = 2 if show_train else 1\n",
    "    n_cols = len(view_data)\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 2, n_rows * 2))\n",
    "\n",
    "    if show_train:\n",
    "        for i in range(n_cols):\n",
    "            axes[0][i].imshow(np.reshape(view_data.data.numpy()[i], (28, 28)), cmap=\"gray\")\n",
    "            axes[0][i].set_xticks(())\n",
    "            axes[0][i].set_yticks(())\n",
    "\n",
    "        for i in range(n_cols):\n",
    "            axes[1][i].clear()\n",
    "            axes[1][i].imshow(np.reshape(gen_data[i], (28, 28)), cmap=\"gray\")\n",
    "            axes[1][i].set_xticks(())\n",
    "            axes[1][i].set_yticks(())\n",
    "\n",
    "    else:\n",
    "        for i in range(n_cols):\n",
    "            axes[i].imshow(np.reshape(gen_data[i], (28, 28)), cmap=\"gray\")\n",
    "            axes[i].set_xticks(())\n",
    "            axes[i].set_yticks(())\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 128\n",
    "transforms = Compose([ToTensor(), Lambda(lambda x: x.flatten())])\n",
    "\n",
    "# Mnist dataset\n",
    "train_data = FashionMNIST(\n",
    "    root=\".\", train=True, transform=transforms, download=True\n",
    ")  # change to false if you already have the data\n",
    "\n",
    "# Data Loader for easy mini-batch return in training, the image batch shape will be (50, 1, 28, 28)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generative Adversarial Networks (GANs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Idea GANów polega na takim ustaleniu wag generatora, aby utworzone przykłady były nieodróżnialne od prawdziwych danych (pochodziły z tego samego rozkładu). Nowatorskie podejście w GANach polega na tym, że model oceniający czy dane są realistyczne, będzie się uczył razem z modelem generującym. Powyższa idea oznacza to, że model GANowski jest grą dwóch graczy:\n",
    "\n",
    "* pierwszym graczem jest generator $G(z)$ reprezentowany przez sieć neuronową z parametrami $\\theta$ zaś $z$ ∼ $N(0, I)$ jest losowym szumem z rozkładu normalnego. Jego głównym zadaniem jest przekształcenie szumu wejściowego $z$ w obiekt $G(z)$ ∼ $P_G$ podobny do rzeczywistych danych (z rozkładu $P_{dane}$).\n",
    "* drugim graczem jest dyskryminator $D(x)$ reprezentowany przez sieć neuronową z parametrami $\\phi$, zaś $x$ jest obiektem z $P_G$ albo $P_{dane}$. Dyskryminator ma odróżniać dane pochodzące od generatora od danych rzeczywistych. Mówiąc dokładniej jest to klasyfikator, który zwraca prawdopodobieństwo, że obiekt $x$ pochodzi z danych rzeczywistych $P_{dane}$, a nie z $P_G$.\n",
    "\n",
    "<img src=\"resources/gan.png\">\n",
    "\n",
    "Architecture of a generative adversarial network. ([Image source](http://www.kdnuggets.com/2017/01/generative-adversarial-networks-hot-topic-machine-learning.html))"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2099283122.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  Cell \u001B[1;32mIn[77], line 1\u001B[1;36m\u001B[0m\n\u001B[1;33m    Idea GANów polega na takim ustaleniu wag generatora, aby utworzone przykłady były nieodróżnialne od prawdziwych danych (pochodziły z tego samego rozkładu). Nowatorskie podejście w GANach polega na tym, że model oceniający czy dane są realistyczne, będzie się uczył razem z modelem generującym. Powyższa idea oznacza to, że model GANowski jest grą dwóch graczy:\u001B[0m\n\u001B[1;37m         ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Zadanie 1. Vanilla GAN (3 pkt.)\n",
    "\n",
    "Należy zaimplementować klasy Generator i Discriminator. Można zastosować dowolną architekturę sieci pod warunkiem, że:\n",
    "* Generator przyjmuje wektor o rozmiarze `latent_dim` i produkuje wektor o rozmiarze `out_dim` z wartościami w zakresie \\[-1, 1\\]. Przykladową implementacją są warstwy nn.Linear \\[latent_dim, 128, 256, 512, out_dim] z aktywacjami nn.LeakyReLU oraz nn.Tanh na końcu.\n",
    "* Discriminator przyjmuje wektor o rozmiarze `input_size` i produkuje wektor o rozmiarze `1` z wartościami w zakresie \\[0, 1\\]. Przykladową implementacją są warstwy nn.Linear \\[input_size, 128, 128, 64, 1] z aktywacjami nn.LeakyReLU oraz nn.Sigmoid na końcu."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Generator(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, latent_dim: int, out_dim: int):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        self.model = ...\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size: int):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.model = ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "epochs: int = 25\n",
    "g_lr: float = 1e-4  # learning rate\n",
    "d_lr: float = 1e-4\n",
    "latent_dim = 64\n",
    "\n",
    "# models\n",
    "generator = Generator(latent_dim, 784).to(device)\n",
    "discriminator = Discriminator(784).to(device)\n",
    "\n",
    "# optimizers\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=g_lr)\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=d_lr)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_losses = []  # For logging purposes\n",
    "    for step, (x, _) in enumerate(train_loader):\n",
    "        real_x = x.to(device)\n",
    "\n",
    "        batch_size = real_x.shape[0]\n",
    "\n",
    "        # Train discriminator\n",
    "        d_optimizer.zero_grad()\n",
    "\n",
    "        real_d_pred = discriminator(real_x)\n",
    "        real_discriminator_loss = criterion(real_d_pred, torch.ones((batch_size, 1), device=device))\n",
    "\n",
    "        noise = torch.randn((batch_size, latent_dim), device=device)\n",
    "        fake_x = generator(noise)\n",
    "        fake_d_pred = discriminator(fake_x.detach())\n",
    "        fake_discriminator_loss = criterion(fake_d_pred, torch.zeros((batch_size, 1), device=device))\n",
    "\n",
    "        discriminator_loss = real_discriminator_loss + fake_discriminator_loss\n",
    "        discriminator_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        # Train generator\n",
    "        g_optimizer.zero_grad()\n",
    "\n",
    "        noise = torch.randn((batch_size, latent_dim), device=device)\n",
    "        gen_x = generator(noise)\n",
    "        gen_d_pred = discriminator(gen_x)\n",
    "        generator_loss = criterion(gen_d_pred, torch.ones((batch_size, 1), device=device))\n",
    "        generator_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        epoch_losses.append(np.array([discriminator_loss.item(), generator_loss.item()]))\n",
    "\n",
    "    epoch_losses_np = np.stack(epoch_losses, axis=0)\n",
    "\n",
    "    print(f\"Epoch: {epoch}  |  total loss: {epoch_losses_np.mean():.4f} |  disc_loss: {epoch_losses_np[:, 0].mean():.4f} | gen_loss: {epoch_losses_np[:, 1].mean():.4f}\")\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        plot_dataset(train_data, generator)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Zadanie 2. Conditional GAN (4 pkt.)\n",
    "\n",
    "Zwykły GAN generuje obraz z szumu i trudno nam kontrolować wynik. Możemy wymusić GAN generować obrazki z wybranej klasy podając do sieci wektor, który ją enkoduje.\n",
    "\n",
    "<img src=\"resources/cgan.png\">\n",
    "\n",
    "CGAN vs GAN diagram ([Image source](https://learnopencv.com/conditional-gan-cgan-in-pytorch-and-tensorflow/))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Należy zaimplementować klasy ConditionalGenerator i ConditionalDiscriminator. Można zastosować dowolną architekturę sieci pod warunkiem, że:\n",
    "* Generator przyjmuje wektory o rozmiarach `latent_dim` i `num_classes` oraz produkuje wektor o rozmiarze `out_dim` z wartościami w zakresie \\[-1, 1\\]. Przykladową implementacją są warstwy nn.Linear \\[latent_dim + num_classes, 128, 256, 512, out_dim] z aktywacjami nn.LeakyReLU oraz nn.Tanh na końcu.\n",
    "* Discriminator przyjmuje wektory o rozmiarach `latent_dim` i `num_classes` oraz produkuje wektor o rozmiarze `1` z wartościami w zakresie \\[0, 1\\]. Przykladową implementacją są warstwy nn.Linear \\[input_size + num_classes, 128, 128, 64, 1] z aktywacjami nn.LeakyReLU oraz nn.Sigmoid na końcu."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ConditionalGenerator(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, latent_dim: int, out_dim: int, num_classes: int):\n",
    "        super(ConditionalGenerator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.model = ...\n",
    "\n",
    "    def forward(self, x, one_hot_label):\n",
    "        x = torch.cat((x, one_hot_label), dim=1)  # może być zmienione\n",
    "        return self.model(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ConditionalDiscriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size: int, num_classes:int):\n",
    "        super(ConditionalDiscriminator, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.model = ...\n",
    "\n",
    "    def forward(self, x, one_hot_label):\n",
    "        x = torch.cat((x, one_hot_label), dim=1) # może być zmienione\n",
    "        return self.model(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "epochs: int = 25\n",
    "g_lr: float = 1e-4\n",
    "d_lr: float = 1e-4\n",
    "latent_dim = 64\n",
    "\n",
    "# models\n",
    "generator = ConditionalGenerator(latent_dim, 784, 10).to(device)\n",
    "discriminator = ConditionalDiscriminator(784, 10).to(device)\n",
    "\n",
    "# optimizers\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=g_lr)\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=d_lr)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_losses = []  # For logging purposes\n",
    "    for step, (x, y) in enumerate(train_loader):\n",
    "        real_x= x.to(device)\n",
    "        y_one_hot = torch.nn.functional.one_hot(y, 10).to(torch.float32).to(device)\n",
    "        batch_size = real_x.shape[0]\n",
    "\n",
    "        # Train discriminator\n",
    "        d_optimizer.zero_grad()\n",
    "\n",
    "        real_d_pred = ...\n",
    "        real_discriminator_loss  = criterion(real_d_pred, torch.ones((batch_size, 1), device=device))\n",
    "\n",
    "        noise = torch.randn((batch_size, latent_dim), device=device)\n",
    "        fake_x = ...\n",
    "        fake_d_pred = ...\n",
    "        fake_discriminator_loss = criterion(fake_d_pred, torch.zeros((batch_size, 1), device=device))\n",
    "\n",
    "        discriminator_loss = real_discriminator_loss + fake_discriminator_loss\n",
    "        discriminator_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        # Train generator\n",
    "        g_optimizer.zero_grad()\n",
    "\n",
    "        noise = torch.randn((batch_size, latent_dim), device=device)\n",
    "        gen_x = ...\n",
    "        gen_d_pred = ...\n",
    "        generator_loss = criterion(gen_d_pred, torch.ones((batch_size, 1), device=device))\n",
    "        generator_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        epoch_losses.append(np.array([discriminator_loss.item(), generator_loss.item()]))\n",
    "\n",
    "    epoch_losses_np = np.stack(epoch_losses, axis=0)\n",
    "\n",
    "    print(f\"Epoch: {epoch}  |  total loss: {epoch_losses_np.mean():.4f} |  disc_loss: {epoch_losses_np[:, 0].mean():.4f} | gen_loss: {epoch_losses_np[:, 1].mean():.4f}\")\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        plot_dataset(train_data, generator, is_cond=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_dataset(train_data, generator, is_cond=True, n_samples=10, show_train=False)  # generuje wszystkie klasy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
