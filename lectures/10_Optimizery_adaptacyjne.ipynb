{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MUM 2023-24 Optymalizatory adaptacyjne"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: right;\" src=\"ml_figures/conv_nn.png\" width=450>\n",
    "\n",
    "* wartoÅ›ci bezwzglÄ™dne gradientÃ³w mogÄ… siÄ™ bardzo rÃ³Å¼niÄ‡ miÄ™dzy warstwami duÅ¼ych sieci neuronowych\n",
    "  * globalne $\\gamma_t$ moÅ¼e nie dziaÅ‚aÄ‡ poprawnie\n",
    "  * Å›ledziÄ‡ i uaktualniaÄ‡ wspÃ³Å‚czynnik uczenia dla kaÅ¼dego parametru\n",
    "  * algorytmy adaptujÄ… siÄ™ do _wariancji_ wag albo lokalnej krzywizny problemu (powierzchni bÅ‚Ä™du)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Uwaga\n",
    "\n",
    "1. W oryginalnych pracach dotyczÄ…cych niektÃ³rych z poniÅ¼szych metod pojawiajÄ… siÄ™ matematyczne \"akrobacje\", ktÃ³re polegajÄ… na przykÅ‚ad na definiowaniu \"pierwiastka ze zdiagonalizowanej macierzy produktu zewnÄ™trznego historii gradientÃ³w\". ProszÄ™ absolutnie nie myÅ›leÄ‡ w ten sposÃ³b o tych optimizerach!\n",
    "\n",
    "2. W tym notebooku wszystkie operacje na wektorach sÄ… zdefiniowane _element-wise_, zgodnie z reguÅ‚ami pakietu numpy. Na przykÅ‚ad:\n",
    "    * kwadrat wektora to wektor kwadratÃ³w elementÃ³w (numpy.square)\n",
    "    * pierwiastek z wektora to wektor pierwiastkÃ³w z elementÃ³w (numpy.sqrt)\n",
    "    * suma, rÃ³Å¼nica, iloraz, iloczyn - analogicznie\n",
    "    * suma wektora i liczby oznacza dodanie tej liczby do kaÅ¼dej wspÃ³Å‚rzÄ™dnej"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adagrad\n",
    "<img style=\"float: right;\" src=\"ml_figures/opt2.gif\" width=450>\n",
    "\n",
    "* $\\gamma_t$ - wspÃ³Å‚czynnik uczenia (learning rate), typowo od 0.001 do 0.01\n",
    "* $\\epsilon$ - zapobiega dzieleniu przez zero (zwykle $10^{-8}$)\n",
    "* $h_t$ - wektor sumujÄ…cy kwadraty gradientÃ³w do czasu $t$, wymiar taki sam jak $w$; $h_t(0)=0$\n",
    "$$\\begin{align}\n",
    "h_{t+1} &= h_t + \\nabla L(w_t)^2\\\\\n",
    "w_{t+1}&=w_t - \\gamma_t \\dfrac{\\nabla L(w_t)}{\\sqrt{h_{t+1} + \\epsilon}}\n",
    "\\end{align}$$\n",
    "1. dzielenie normalizuje gradient\n",
    "2. normalizacja oddzielnie dla:\n",
    "    * kaÅ¼dej wspÃ³Å‚rzÄ™dnej gradientu\n",
    "      * czyli dla kaÅ¼dej wspÃ³Å‚rzÄ™dnej $w$ - osobno dla kaÅ¼dego parametru modelu\n",
    "3. $h_t$ jest sumÄ… kwadratÃ³w, wiÄ™c trzeba w mianowniku wziÄ…Ä‡ pierwiastek - bez pierwiastka dziaÅ‚a sÅ‚abo\n",
    "4. __akumulacja__ kwadratÃ³w gradientÃ³w\n",
    "    * $h$ to suma, a nie Å›rednia\n",
    "    * gdyby gradienty byÅ‚y staÅ‚e, mianownik rÃ³sÅ‚by proporcjonalnie do $\\sqrt{t}$\n",
    "    * w praktyce maleje $\\Delta w$ i model moÅ¼e __przestaÄ‡ siÄ™ uczyÄ‡__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp (Hinton)\n",
    "<img style=\"float: right;\" src=\"ml_figures/opt2.gif\" width=450>\n",
    "\n",
    "* normalizacja przez pierwiastek wartoÅ›ci oczekiwanej gradientu\n",
    "* $\\gamma$ - learning rate (typowe wartoÅ›ci: od 0.001 do 0.01)\n",
    "* $\\alpha$ - wspÃ³Å‚czynnik Å›redniej kroczÄ…cej (zazwyczaj: 0.9)\n",
    "* $\\epsilon$ - zapobiega dzieleniu przez zero (zazwyczaj: $10^{-8})$\n",
    "* $v_t$ - wektor Å›redniej kroczÄ…cej kwadratÃ³w gradientÃ³w do czasu $t$, wymiar taki sam jak $w$\n",
    "  * $v_t$ jest estymacjÄ… drugiego momentu\n",
    "  * wiÄ™kszoÅ›Ä‡ algorytmÃ³w uÅ¼ywa Å›rednich kroczÄ…cych dla estymacji szumu, ktÃ³ry zmienia siÄ™ w czasie\n",
    "$$\\begin{align}\n",
    "v_{t+1} &= \\overbrace{\\alpha v_{t} + \\overbrace{(1-\\alpha)\\nabla L(w_t)^2}^{\\textrm{kwadrat po elementach (element-wise)}}}^{\\textrm{wykÅ‚adnicza Å›rednia kroczÄ…ca}}\\\\\n",
    "w_{t+1}&=w_t - \\gamma\\frac{\\nabla L(w_t)}{\\sqrt{v_{t+1}} + \\epsilon}\n",
    "\\end{align}$$\n",
    "  * $\\epsilon$ zabezpiecza przed dzieleniem przez zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adadelta\n",
    "<img style=\"float: right;\" src=\"ml_figures/opt2.gif\" width=450>\n",
    "\n",
    "* $\\alpha$ - wspÃ³Å‚czynnik Å›redniej kroczÄ…cej (zazwyczaj: 0.95)\n",
    "* $\\epsilon$ - zapobiega dzieleniu przez zero, umoÅ¼liwia rozpoczÄ™cie uczenia (zazwyczaj: od $10^{-6}$ do $10^{-2}$)\n",
    "\n",
    "* $v_t$ - wektor Å›redniej kroczÄ…cej kwadratÃ³w gradientÃ³w do czasu $t$ element po elemencie\n",
    "* $d_t$ - wektor Å›redniej kroczÄ…cej $\\Delta{}w$ do czasu $t$ element po elemencie\n",
    "$$\\begin{align}\n",
    "v_{t+1} &= \\alpha{}v_t + (1-\\alpha)(\\nabla L(w_t)^2\\\\\n",
    "w_{t+1} &=w_t - \\sqrt{d_t + \\epsilon}\\; \\dfrac{\\nabla L(w_t)}{\\sqrt{v_{t+1} + \\epsilon}}\\\\\n",
    "d_{t+1} &= \\alpha d_t + (1-\\alpha)(w_{t+1} - w_{t})^2\n",
    "\\end{align}$$\n",
    "1. rozszerzenie RMSProp (ale zaproponowane niezaleÅ¼nie jako poprawka Adagrad)\n",
    "2. zastÄ…pienie learning rate przez __wektor__\n",
    "    * learning rate __proporcjonalny do__ Å›redniego $\\Delta{}w$\n",
    "    * upodobnienie szybkoÅ›ci poprawek do poprawek\n",
    "    * __eliminacja__ staÅ‚ej uczenia z parametrÃ³w\n",
    "3. waÅ¼na rola parametru $\\epsilon$\n",
    "    * nie tylko zapobiega dzieleniu przez zero\n",
    "    * umoÅ¼liwia rozpoczÄ™cie uczenia - $d_1$ jest wiÄ™ksze od zera\n",
    "    * $\\sqrt{\\epsilon}$ wyznacza __dolne ograniczenie__ $\\sqrt{d_t +\\epsilon}$, a stÄ…d uczenie nie zatrzymuje siÄ™"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Adam: a method for stochastic optimization, D.P. Kingma, J. Ba, 2015](https://arxiv.org/pdf/1412.6980.pdf)\n",
    "* rodzaj RMSprop z elementem momentum\n",
    "* $\\gamma$ - learning rate (zazwyczaj: 0.001)\n",
    "* $\\beta_1$ - wspÃ³Å‚czynnik Å›redniej kroczÄ…cej pierwszego momentu (zazwyczaj: 0.9)\n",
    "* $\\beta_2$ - wspÃ³Å‚czynnik Å›redniej kroczÄ…cej pierwszego momentu (zazwyczaj: 0.999)\n",
    "* $\\epsilon$ - zapobiega dzieleniu przez zero (zazwyczaj: $10^{-8}$)\n",
    "* $m_t$ - wektor Å›redniej kroczÄ…cej pierwszego momentu gradientu do czasu $t$\n",
    "* $v_t$ - wektor Å›redniej kroczÄ…cej drugiego momentu gradientu do czasu $t$\n",
    "$$\\begin{align}\n",
    "m_{t+1} &= \\beta_1 m_t + (1-\\beta_1)(\\nabla L(w_t)\\\\\n",
    "v_{t+1} &= \\beta_2 v_t + (1-\\beta_2)(\\nabla L(w_t))^2\\\\\n",
    "\\widehat m &= \\dfrac{m_{t+1}}{1-\\beta_1^{(t+1)}}\\\\\n",
    "\\widehat v &= \\dfrac{v_{t+1}}{1-\\beta_2^{(t+1)}}\\\\\n",
    "w_{t+1} &= w_t - \\gamma\\dfrac{\\widehat m}{\\sqrt{\\widehat v} + \\epsilon}\n",
    "\\end{align}$$\n",
    "__Uwaga__ $\\beta^{t+1}$ to \"$\\beta$ do potÄ™gi $t+1$\", a nie $\\beta$ w czasie $t+1$\n",
    "albo skrÃ³towo w stabilnej wersji, ktÃ³ra jest szybko osiÄ…gana\n",
    "$$\\begin{align}\n",
    "m_{t+1} &= \\beta_1 m_t + (1-\\beta_1)(\\nabla L(w_t)\\\\\n",
    "v_{t+1} &= \\beta_2 v_t + (1-\\beta_2)(\\nabla L(w_t))^2\\\\\n",
    "w_{t+1} &= w_t - \\gamma\\dfrac{m_t}{\\sqrt{v_{t+1}} + \\epsilon}\n",
    "\\end{align}$$\n",
    "  * do aktualizacji wag uÅ¼ywany jest wektor estymujÄ…cy $m_t$ przez Å›redniÄ… kroczÄ…cÄ…\n",
    "    * Å›rednia kroczÄ…ca __zbiasowana__ w kierunku zera\n",
    "      * \"uÅ›rednienia\" wprowadzajÄ… poprawkÄ™: im pÃ³Åºniej, tym poprawka mniejsza\n",
    "  * __gradient__ adaptowany\n",
    "    * kroczÄ…ca Å›rednia gradientÃ³w (pierwszy moment) - tÅ‚umione oscylacje\n",
    "    * kroczÄ…ca Å›rednia kwadratÃ³w gradientÃ³w (drugi moment) - normalizacja gradientÃ³w\n",
    "  * eksponencjalna Å›rednia kroczÄ…ca estymujÄ…ca momentum jest rÃ³wnowaÅ¼na standardowej postaci przy przeskalowaniu\n",
    "\n",
    "* Adam jest zwykle znacznie lepszy od SGD dla problemÃ³w, ktÃ³re sÄ… Åºle uwarunkowane, a zwykle sÄ… ğŸ˜\n",
    "  * czÄ™sto jednak Adam nie zbiega dla prostych problemÃ³w!\n",
    "* Adam ma przewagÄ™ nad RMSprop ze wzglÄ™du na uwzglÄ™dnienie momentum\n",
    "* Adam nie jest dobrze zrozumiaÅ‚y teoretycznie\n",
    "* dla wielu problemÃ³w zwiÄ…zanych z obrazami, na przykÅ‚ad ImageNet, daje gorsze wyniki generalizacji\n",
    "* wymaga wiÄ™cej pamiÄ™ci niÅ¼ SGD\n",
    "* ma dwa parametry momentum, skÄ…d moÅ¼e byÄ‡ potrzebne trochÄ™ tunowania\n",
    "  * zwykle warto wyprÃ³bowaÄ‡ SGD z momentu oraz Adama z szeregiem rÃ³Å¼nych parametrÃ³w by wybraÄ‡\n",
    "\n",
    "[Wizualizacja uczenia dla wielu algorytmÃ³w [deeplearning.ai]](https://www.deeplearning.ai/ai-notes/optimization/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I wiele innych\n",
    "\n",
    "* AdaMax, Nadam, AMSGrad, ...\n",
    "* algorytmy uczenia mogÄ… byÄ‡ specyficzne dla konkretnych architektur i zadaÅ„\n",
    "  * na przykÅ‚ad uczenie sprzÄ™towych architektur wprost\n",
    "    * bardzo rzadko â€” nie sÄ… one dostosowane do uczenia\n",
    "    * implementacja uczenia i inferencji bardzo siÄ™ rÃ³Å¼niÄ… od siebie\n",
    "    * znacznie proÅ›ciej jest uczyÄ‡ symulowanÄ… architekturÄ… a znalezione wagi skopiowaÄ‡\n",
    "* uczenie NN nie musi byÄ‡ gradientowe\n",
    "  * gradientowe wymaga by funkcja kosztu byÅ‚a rÃ³Å¼niczkowalna\n",
    "    * z tego powodu nie moÅ¼emy uczyÄ‡ wprost by celem byÅ‚a jak najlepsza klasyfikacja\n",
    "* alternatywÄ… jest uczenie _perturbacyjne_\n",
    "  * dla aktualnego przykÅ‚adu wagi sÄ… _perturbowane_ o maÅ‚e $-\\epsilon$ i $+\\epsilon$\n",
    "  * wybierana jest poprawka, ktÃ³ra daje lepszy wynik\n",
    "* wiele zaawansowanych metod wykorzystywanych w pÅ‚ytszych sieciach neuronowych nie jest wykorzystywanych dla sieci gÅ‚Ä™bokich\n",
    "  * metody przybliÅ¼ania drugiej pochodnej nie dajÄ… wiele w porÃ³wnaniu do metod bÄ™dÄ…cych uogÃ³lnieniami metody momentum\n",
    "  * wykorzystanie macierzy Hesjanu jest bardzo trudne ze wzglÄ™du na duÅ¼Ä… liczbÄ™ parametrÃ³w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## niektÃ³re cechy pozwalajÄ…ce na pierwszÄ… decyzjÄ™ o wyborze algorytmu [na podstawie deeplearning.ai]\n",
    "[Wizualizacja uczenia dla wielu algorytmÃ³w [deeplearning.ai]](https://www.deeplearning.ai/ai-notes/optimization/index.html)\n",
    "* **SGD**\n",
    "  * proste urÃ³wnoleglenie, jednak powolne gdy pamiÄ™Ä‡ GPU jest niewystarczajÄ…ca\n",
    "  * SGD szybciej zbiega na duÅ¼ych zbiorach danych ze wzglÄ™du na czÄ™stszÄ… aktualizacjÄ™\n",
    "  * lepsza aproksymacja gradientu ze wzglÄ™du na wykorzystanie redundancji danych\n",
    "  * najmniej uÅ¼ytej pamiÄ™ci dla ustalonej wielkoÅ›ci batcha\n",
    "* **momentum**\n",
    "  * przyspiesza uczenie, wykorzystujÄ…c minimalnÄ… modyfikacjÄ™ algorytmu\n",
    "  * wiÄ™cej pamiÄ™ci na batch niÅ¼ SGD, ale sporo mniej niÅ¼ RMSprop czy Adam\n",
    "* **RMSprop**\n",
    "  * adaptacyjne podejÅ›cie zapobiega zbyt szybkiemu zanikaniu czy wybuchowi hiperparametru uczenia\n",
    "  * utrzymuje prÄ™dkoÅ›ci uczenia odpowiednie dla kaÅ¼dego parametru modelu (wagi sieci)\n",
    "  * wiÄ™cej pamiÄ™ci niÅ¼ momentum, ale mniej niÅ¼ Adam\n",
    "* **Adam**\n",
    "  * hiperparametry algorytmu mogÄ… byÄ‡ zwykle ustawione na wartoÅ›ci domyÅ›lne i nie potrzebujÄ… dodatkowego dopasowania\n",
    "    * a jest ich wiele â€” staÅ‚a uczenia, eksponencjalna staÅ‚a zanikania dla momentum, etc.\n",
    "  * realizuje formÄ™ statystycznego podejÅ›cia **wyÅ¼arzania** (ang. annealing) z adaptacyjnymi krokami uczenia\n",
    "  * zuÅ¼ywa najwiÄ™cej pamiÄ™ci na batch\n",
    "  * jest zwykle domyÅ›lnym optymalizatorem\n",
    "    * moÅ¼e byÄ‡ waÅ¼ne przy porÃ³wnywaniu rozwiÄ…zaÅ„"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
