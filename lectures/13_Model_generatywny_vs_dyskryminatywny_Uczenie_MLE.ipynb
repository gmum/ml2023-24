{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MUM 2023-24 Modele generatywne vs dyskryminatywne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Będziemy zawsze pisali $p$, co oznacza:\n",
    "* prawdopodobieństwo - w przypadku dyskretnym\n",
    "* wartość funkcji gęstości - w przypadku ciągłym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model generatywny\n",
    "\n",
    "<img style=\"float: left;\" src=\"ml_figures/generative-model.png\" width=400> \n",
    "\n",
    "1. uczenie nienadzorowane, _unsupervised learning_\n",
    "2. dane $$\\{\\mathbf{x}_n\\}$$\n",
    "    * $\\mathbf{x}$ - wektor __przykładów__\n",
    "        * wartości ciągłe - liczby rzeczywiste\n",
    "        * wartości dyskretne - np. liczby całkowite\n",
    "3. rozkład prawdopodobieństwa\n",
    "    * dane treningowe to __sample z rozkładu__ $p(\\mathbf{x})$\n",
    "    * model uczy się __rozkładu prawdopodobieństwa__ $p(\\mathbf{x})$\n",
    "4. generatywność\n",
    "    * model __nie musi__ umieć __generować__ (samplować) nowych danych (ale zazwyczaj potrafi)\n",
    "    * model __nie musi__ umieć podać wartości liczbowej $p(\\mathbf{x})$ (ale wtedy powinien umieć przynajmniej samplować)\n",
    "5. najpopularniejsze modele\n",
    "    * Variational Auto-Encoder (VAE)\n",
    "    * Generative Adversarial Networks (GAN)\n",
    "    * dziesiątki różnych wariantów\n",
    "6. spektakularne przykłady\n",
    "    * https://blog.openai.com/glow/\n",
    "    * https://deepmind.com/blog/neural-scene-representation-and-rendering/\n",
    "    * https://deepmind.com/blog/wavenet-generative-model-raw-audio/\n",
    "    * https://towardsdatascience.com/auto-regressive-generative-models-pixelrnn-pixelcnn-32d192911173\n",
    "    * https://affinelayer.com/pixsrv/\n",
    "    * https://junyanz.github.io/CycleGAN/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model dyskryminatywny\n",
    "\n",
    "<img style=\"float: left;\" src=\"ml_figures/discriminative-model.png\" width=400> \n",
    "\n",
    "1. uczenie nadzorowane, _supervised learning_\n",
    "2. dane $$\\{(\\mathbf{x}_n,y_n)\\}$$ lub $$\\{(\\mathbf{x}_n,\\mathbf{y}_n)\\}$$\n",
    "    * $\\mathbf{x}$ - wektor __cech__\n",
    "        * cechy ciągłe - liczby rzeczywiste\n",
    "        * cechy dyskretne - np. liczby całkowite\n",
    "    * $y$ - __etykieta__\n",
    "        * etykieta dyskretna, np. __klasa__ w __problemie klasyfikacji__\n",
    "        * etykieta ciągła, np. liczba rzeczywista w __problemie regresji__\n",
    "    * $\\mathbf{y}$ - __wektor etykiet__\n",
    "        * __multilabel classification__ (nie mylić z klasyfikacją wieloklasową)\n",
    "        * __multioutput regression__\n",
    "3. konkretny wektor $\\mathbf{x}$ może mieć __wiele różnych \"prawdziwych\"__ wartości $y$\n",
    "    * np. zdjęcia ręcznie pisanych cyfr - ten sam obrazek może oznaczać $1$ lub $7$\n",
    "    * \"prawdziwa\" wartość zależy od osoby, która pisała, nie od obrazka\n",
    "    * dlatego chcemy $p(y\\mid \\mathbf{x})$, a nie funkcję $\\mathbf{x} \\mapsto y$\n",
    "4. rozkład prawdopodobieństwa\n",
    "    * dane treningowe to __sample z rozkładu łącznego__ $p(\\mathbf{x},y)$\n",
    "    * model uczy się __tylko rozkładu warunkowego__ $p(y\\mid \\mathbf{x})$\n",
    "    * model __nie__ uczy się rozkładu brzegowego $p(\\mathbf{x})$ ani rozkładu łącznego $p(\\mathbf{x}, y)$\n",
    "    * ale model może lepiej nauczyć się przewidywać etykiety częstych wartości $\\mathbf{x}$, ponieważ jest ich więcej w danych treningowych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funkcja dyskryminatywna\n",
    "1. funkcja __dyskryminatywna__\n",
    "    * ma za zadanie zwrócić __jedną konkretną wartość__ $\\widehat{y}$ dla ustalonego $\\mathbf{x}$\n",
    "    * zakładamy, że znamy rozkład $p(y\\mid x)$\n",
    "    * występuje w połączeniu z pewną miarą oceny skuteczności modelu\n",
    "\n",
    "2. przykład - maksymalizowanie __accuracy__ w problemie klasyfikacji\n",
    "$$\\widehat{y} = \\arg\\max_a p(y = a \\mid \\mathbf{x})$$\n",
    "    * zawsze wybieramy najbardziej prawdopodobną klasę\n",
    "    * w ten sposób mylimy się najrzadziej\n",
    "    * __granica decyzyjna__\n",
    "        * zbiór $\\mathbf{x}$ dla których następuje __zmiana decyzji__ $\\widehat{y}$\n",
    "        * w problemie dwuklasowym prawdopodobieństwo wynosi tam 50% / 50%\n",
    "\n",
    "3. przykład - minimalizowanie __mean squared error__ w problemie regresji\n",
    "$$\\widehat{y} = \\mathbb{E}(y\\mid \\mathbf{x})$$\n",
    "    * zawsze wybieramy wartość oczekiwaną pod warunkiem $\\mathbf{x}$\n",
    "    * to optymalizuje błąd średniokwadratowy\n",
    "    * ale uwaga - inne miary wymagają innego podejścia (co zrobić np. w wypadku __mean absolute error__? dlaczego?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Można jeszcze inaczej\n",
    "* uczenie **częściowo nadzorowane** (_semi-supervised_)\n",
    "    * połączenie modelu generatywnego i dyskryminatywnego\n",
    "    * dużo danych __nieetykietowanych__, mało __etykietowanych__ (kosztowne etykietowanie)\n",
    "    * chcemy wykorzystać wszystkie dane\n",
    "* **klastrowanie** (uczenie nienadzorowane)\n",
    "    * ekspert może określić klasy dla skupisk\n",
    " \n",
    "<img style=\"float: left;\" src=\"ml_figures/mnist_aae_latent.png\" width=350>\n",
    "\n",
    "* **uczenie generatywne**\n",
    "  * model generatywny jak wyżej, a więc modelujący rozkład prawdopodobieństwa danych\n",
    "  * umiejący jednak generować (samplować, próbkować) przykłady z tego rozkładu\n",
    "    * prosty przykład modeluje rozkład\n",
    "    * nie ma jednak generatora losowego z tego rozkładu\n",
    "---\n",
    "* agent w ***reinforcement learning*** (zasada podobna do modeli dyskryminatywnych)\n",
    "    * robot przewiduje właściwą akcję $y$ na podstawie obserwacji $\\mathbf{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modele\n",
    "\n",
    "1. najpopularniejsze modele występują w dwóch wariantach - klasyfikator i regressor:\n",
    "    * naiwny Bayesowski\n",
    "    * liniowy\n",
    "    * KNN\n",
    "    * Support Vector Machines SVM\n",
    "    * drzewa decyzyjne i lasy drzew\n",
    "    * drzewa gradient boosted\n",
    "    * modele składające wyniki, np. AdaBoost\n",
    "    * **sieci neuronowe**\n",
    "2. modele generatywne to zazwyczaj\n",
    "    * proste modele gęstościowe\n",
    "    * głębokie sieci neuronowe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
