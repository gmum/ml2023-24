{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MUM 2023-24 Regresja liniowa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motywacja\n",
    "\n",
    "1. mamy już\n",
    "  * dane treningowe\n",
    "  * optimizery\n",
    "  * funkcję kosztu\n",
    "\n",
    "2. wiemy, co powinien robić model $M$\n",
    "  * działanie $M$ kontrolują parametry $\\theta$\n",
    "  * $M_\\theta$ to funkcja, która przyjmuje wektor $\\mathbf{x}$ i zwraca liczbę $\\widehat{y}$\n",
    "\n",
    "3. chcemy zdefiniować $M$ tak, aby dla różnych $\\theta$ mógł modelować wiele różnych zależności $y$ od $\\mathbf{x}$\n",
    "\n",
    "4. najprostszy sensowny model to model liniowy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model liniowy\n",
    "1. definicja\n",
    "$$\\begin{align}\n",
    "\\mathbf{x} &= (x_1, \\ldots, x_D) \\\\\n",
    "\\theta &= (\\theta_0, \\theta_1, \\ldots, \\theta_D) \\\\\n",
    "\\widehat{y}&=M_\\theta(\\mathbf{x})\\\\\n",
    "  &=\\theta_0+\\theta_1x_1+\\dots+\\theta_Dx_D\\\\\n",
    "  &=\\langle[1, \\mathbf{x}], \\theta\\rangle\n",
    "  \\end{align}$$\n",
    "2. $\\theta$ jest dłuższe o jedną współrzędną od $\\mathbf{x}$\n",
    "3. $[1, \\mathbf{x}]$ to __rozszerzony wektor wejściowy__\n",
    "  * dodatkowa współrzędna - dopisujemy jedynkę na początku wektora\n",
    "  * upraszcza notację - możemy użyć iloczynu skalarnego\n",
    "4. $\\theta_0$ nazywane jest __bias__\n",
    "  * trochę niefortunna nazwa, ale tak się przyjęło\n",
    "5. liniowy, ponieważ kombinacja liniowa\n",
    "  * tak naprawdę to afiniczny, bo dodajemy $\\theta_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funkcja kosztu\n",
    "\n",
    "1. definiujemy funkcję kosztu jako mean squared error\n",
    "2. $N$ elementowy zbiór treningowy\n",
    "3. policzmy funkcję kosztu względem parametrów $\\theta$\n",
    "$$\\begin{align}\n",
    "L(\\theta) &= \\dfrac{1}{N}\\sum_{n=1}^N(y_n - \\widehat{y_n})^2 \\\\\n",
    " &= \\dfrac{1}{N}\\sum_{n=1}^N(y_n - \\langle[1,\\mathbf{x}_n], \\theta\\rangle)^2\n",
    "\\end{align}$$\n",
    "4. gradient $L$ względem $\\theta$ (__rozpisać na ćwiczeniach__)\n",
    "$$\\begin{align}\n",
    "\\nabla L(\\theta) &= \\nabla\\dfrac{1}{N}\\sum_{n=1}^N(y_n - \\langle[1,\\mathbf{x}_n], \\theta\\rangle)^2 \\\\\n",
    " &= \\ldots \\\\\n",
    " &= \\dfrac{2}{N}\\sum_{n=1}^N(\\langle[1,\\mathbf{x}_n], \\theta\\rangle - y_n)\\cdot [1,\\mathbf{x}_n] \\\\\n",
    " &= \\dfrac{2}{N}\\sum_{n=1}^N(\\widehat{y_n} - y_n)\\cdot [1,\\mathbf{x}_n] \\\\\n",
    "\\end{align}$$\n",
    "  * co tutaj jest liczbą, a co wektorem?\n",
    "5. szukamy minimum\n",
    "  * wprowadźmy najpierw pewne oznaczenia\n",
    "    * $[1,X]$ - macierz wszystkich rozszerzonych wektorów $[1,\\mathbf{x}_n]$\n",
    "      * $N$ wierszy, $D+1$ kolumn\n",
    "      * każdy przykład w jednym wierszu\n",
    "      * pierwsza kolumna składa sie z samych (doklejonych) jedynek\n",
    "    * $Y$ - macierz wszystkich etykiet $y_n$\n",
    "      * $N$ wierszy, jedna kolumna\n",
    "      * każda etykieta w jednym wierszu\n",
    "  * przyrównujemy gradient do zera\n",
    "$$\\begin{align}\n",
    "\\nabla L(\\widehat\\theta) &= 0 \\\\\n",
    "\\dfrac{2}{N}\\sum_{n=1}^N(\\langle[1,\\mathbf{x}_n], \\widehat\\theta\\rangle - y_n)\\cdot [1,\\mathbf{x}_n] &= 0 \\\\\n",
    "\\sum_{n=1}^N(\\langle[1,\\mathbf{x}_n], \\widehat\\theta\\rangle - y_n)\\cdot [1,\\mathbf{x}_n] &= 0 \\\\\n",
    "\\sum_{n=1}^N[1,\\mathbf{x}_n]\\cdot\\langle[1,\\mathbf{x}_n], \\widehat\\theta\\rangle &= \\sum_{n=1}^N[1,\\mathbf{x}_n]\\cdot y_n\\\\\n",
    "[1,X]^T\\cdot[1,X]\\cdot\\widehat\\theta &= [1,X]^T\\cdot Y \\\\\n",
    "\\end{align}$$\n",
    "$$\\boxed{\n",
    "\\widehat\\theta = ([1,X]^T\\cdot[1,X])^{-1}\\cdot[1,X]^T\\cdot Y}$$\n",
    "  * uwaga - ostatnia równość wymaga istnienia $([1,X]^T\\cdot[1,X])^{-1}$\n",
    "    * co jeśli ta macierz nie jest odwracalna? czy to znaczy, że funkcja kosztu nie ma minimum?\n",
    "  * jakie wymiary (i dlaczego) mają poszczególne macierze?\n",
    "  * $\\widehat\\theta$ jest wektorem poziomym czy pionowym?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problemy\n",
    "1. jeśli $([1,X]^T\\cdot[1,X])^{-1}$ istnieje, to\n",
    "  * funkcja kosztu jest wypukła\n",
    "  * ma dokładnie jedno minimum globalne\n",
    "  * prosty wzór na uczenie modelu\n",
    "2. ale\n",
    "  * wymaga obliczenia odwrotności macierzy\n",
    "    * macierz ma wymiar $D+1\\times D+1$\n",
    "    * jeśli $D$ (liczba cech) jest duże\n",
    "      * odwracanie macierzy jest złożone obliczeniowo rzędu O($D^3$)\n",
    "      * problemy z pamięcią\n",
    "      * istnieją metody redukcji wymiarowości cech\n",
    "3. jeśli $([1,X]^T\\cdot[1,X])^{-1}$ nie istnieje lub $D$ jest duże\n",
    "  * można optymalizować gradientowo\n",
    "    * jeśli sami piszemy kod optymalizacji, można go testować dla małych $D$ stosując jawny wzór"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
