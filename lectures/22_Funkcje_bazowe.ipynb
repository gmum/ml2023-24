{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MUM 2023-24 Funkcje bazowe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motywacja\n",
    "\n",
    "1. model liniowy jest za słaby\n",
    "  * np. klasyfikacja punktów na bliskie i dalekie od zera\n",
    "    * nieliniowa granica decyzyjna\n",
    "2. wiele nieliniowych hipotez, których nie możemy wziąć pod uwagę\n",
    "3. proste rozwiązanie - zdefiniować nieliniową funkcję $\\Phi$\n",
    "$$\\begin{align} \n",
    "\\Phi &: \\mathbb{R}^D \\ni \\mathbf{x} \\mapsto \\Phi(\\mathbf{x}) \\in \\mathbb{R}^J\\\\\n",
    "\\Phi &= (\\phi_1, \\ldots, \\phi_J)\n",
    "\\end{align}$$ przy czym zwykle $J>D$\n",
    "4. $\\phi_j$ to __funkcja bazowa__\n",
    "  * tworzy nową (nieliniową) __cechę__ na podstawie __całego wektora__ $\\mathbf{x}$\n",
    "  * łącznie $J$ nowych cech (zamiast $D$ starych)\n",
    "  * zwykle dodajemy $\\phi_0=1$, pozwalające na dodanie \"biasu\" do modelu\n",
    "  * model uczy się $$y=^T\\Phi(x)=w_0\\phi_0(x)+w_1\\phi_1(x)+\\dots+w_J\\phi_J(x)$$\n",
    "    * można powiedzieć, że uzyskujemy nieliniowy model wykorzystując metody regresji liniowej\n",
    "5. dla nieliniowej funkcji $\\Phi$ i $J>D$ dostajemy tzw. **model Pao**, który teoretycznie jest w stanie rozwiązać dowolne zadanie z dowolną dokładnością\n",
    "  * patrz ogólne twierdzenie o sieciach z jedną warstwą ukrytą\n",
    "  * zwykle płacimy za to słabymi zdolnościami do generalizacji\n",
    "  * potrzebny jest bardzo dobrze zaprojektowany proces uczenia ze zbiorami walidacyjnymi, poszukiwaniem hiperparametrów, ostrą regularyzacją, dużym zbiorem uczącym\n",
    "5. zbyt duże $J$ stwarza problemy\n",
    "  * dobrze jest, gdy $N >> J$ (czyli też $N >> D$)\n",
    "6. $\\Phi$ musi zostać zdefiniowane __przed uczeniem__\n",
    "  * \"udawany\" nieliniowy model\n",
    "  * model tak naprawdę wciąż jest liniowy, bo __nie może sam nauczyć się $\\Phi$__\n",
    "    * model jest liniowy w przestrzeni parametrów $\\theta$, co ułatwia analizę modelu\n",
    "  * model wciąż niekoniecznie będzie mógł znaleźć prawdziwe rozwiązanie\n",
    "  * ale $\\Phi$ pozwala nam dodać do modelu __wiedzę ekspercką__\n",
    "7. model, który uczy się $\\Phi$ - sieć neuronowa\n",
    "  * w teorii - __uniwersalny aproksymator__\n",
    "    * może nauczyć się dowolnego rozkładu prawdopodobieństwa\n",
    "  * w praktyce\n",
    "    * sieć musiałaby być nieskończenie duża\n",
    "    * optymalizacja gradientowa niekoniecznie znajdzie minimum globalne\n",
    "    * a nawet jak znajdzie, to MLE wcale nie jest takie dobre\n",
    "    * ale wciąż to jeden z najlepszych modeli (jeśli umie się go stabilnie trenować)\n",
    "8. innym podejściem, podobnym, może być zdefiniowanie funkcji ekstrahujących cechy bezpośrednio związane z problemem\n",
    "  * **plus** nasz model nie musi się uczyć\n",
    "  * **minus** możemy źle wybrać istotne cechy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przykłady\n",
    "\n",
    "1. często funkcje wielomianowe $\\phi_j(x)=x^j$\n",
    "  * wielomiany są funkcjami globalnymi\n",
    "  * zmiany w jednym obszarze wpływają na wszystkie inne\n",
    "  * użycie wielomianów jest uzasadnione tym, że każda ciągła funkcja może być dowolnie dokładnie aproksymowana wielomianami\n",
    "2. funkcje o postaci $$x_1^{p_1}x_2^{p_2}\\dots x_D^{p_D}$$ z ograniczeniem $p_1+p_2+\\dots+p_D\\leq r$\n",
    "  * razem $$J=\\frac{(D+r)!}{D!\\,r!}$$ funkcji\n",
    "  * $D=10$ i $r=3$ daje $J=286$\n",
    "    * bardzo dużo parametrów do ustalenia\n",
    "      * tylko ostra regularyzacja usunie redundantne\n",
    "  * to są funkcje __globalne__\n",
    "    * zmiany w jednym obszarze danych wpływają na wszystkie inne\n",
    "3. inne to funkcje __spline__\n",
    "  * podział na podobszary i różne wielomiany w nich\n",
    "4. częste są funkcje __lokalne__\n",
    "  * gausowskie \n",
    "    $$\\phi_j(x)=\\exp\\left(-\\frac{(x-\\mu_j)^2}{2\\sigma^2}\\right)$$\n",
    "    * wartości $\\mu_j$\n",
    "      * równo rozłożone\n",
    "      * centroidy klastrów\n",
    "    * sigmoidalne $$\\phi_j(x)=\\sigma\\left(\\frac{x-\\mu_j}{a}\\right)$$\n",
    "5. także Fourierowskie i wavelets\n",
    "  * szczególnie dla przetwarzania sygnałów\n",
    "6. sklejane, radialne, furierowskie, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
